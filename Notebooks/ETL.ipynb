{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from rake_nltk import Rake\n",
    "from spacy.matcher import Matcher \n",
    "import en_core_web_lg\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['./data/biorxiv_medrxiv/', './data/comm_use_subset/','./data/custom_license/', './data/noncomm_use_subset/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for directory in directories:\n",
    "    for (dirpath, dirnames, filenames) in os.walk(directory):\n",
    "        filenames = [names for names in filenames if '.json' in names]\n",
    "        if filenames != []:\n",
    "            files.append({'dirpath':dirpath, 'filenames':filenames})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(txt):\n",
    "    txt=re.sub(r'\\n','',txt)\n",
    "    txt=re.sub(' +', ' ', txt)\n",
    "    txt=re.sub(',', '', txt)\n",
    "    txt=re.sub(r'\\([^()]*\\)','',txt)\n",
    "    txt=re.sub(r'https?:\\S+\\sdoi','',txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pub_extract(data):\n",
    "    abstract = ''\n",
    "    for section in data['abstract']:\n",
    "        abstract = abstract + '  ' +  section['text']\n",
    "    abstract = clean(abstract)\n",
    "    \n",
    "    text = ''\n",
    "    for section in data['body_text']:\n",
    "        text = text + '  ' + section['text']\n",
    "    text = clean(text)\n",
    "    ID = data['paper_id']\n",
    "    title = data['metadata']['title']\n",
    "    return ID, title, abstract, text\n",
    "\n",
    "\n",
    "\n",
    "def phrases_extract(text):\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(text)\n",
    "    key_phrase = r.get_ranked_phrases() # To get keyword phrases ranked highest to lowest.\n",
    "    \n",
    "    return key_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "    ## chunk 1\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "    prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "    prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "\n",
    "  #############################################################\n",
    "  \n",
    "    for tok in nlp(sent):\n",
    "        ## chunk 2\n",
    "        # if token is a punctuation mark then move on to the next token\n",
    "        if tok.dep_ != \"punct\":\n",
    "          # check: token is a compound word or not\n",
    "          if tok.dep_ == \"compound\":\n",
    "            prefix = tok.text\n",
    "            # if the previous word was also a 'compound' then add the current word to it\n",
    "            if prv_tok_dep == \"compound\":\n",
    "                   prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # check: token is a modifier or not\n",
    "        if tok.dep_.endswith(\"mod\") == True:\n",
    "            modifier = tok.text\n",
    "            # if the previous word was also a 'compound' then add the current word to it\n",
    "            if prv_tok_dep == \"compound\":\n",
    "                modifier = prv_tok_text + \" \"+ tok.text\n",
    "\n",
    "          ## chunk 3\n",
    "        if tok.dep_.find(\"subj\") == True:\n",
    "            ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "            prefix = \"\"\n",
    "            modifier = \"\"\n",
    "            prv_tok_dep = \"\"\n",
    "            prv_tok_text = \"\"      \n",
    "\n",
    "          ## chunk 4\n",
    "        if tok.dep_.find(\"obj\") == True:\n",
    "            ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "\n",
    "          ## chunk 5  \n",
    "          # update variables\n",
    "        prv_tok_dep = tok.dep_\n",
    "        prv_tok_text = tok.text\n",
    "  #############################################################\n",
    "\n",
    "    return [ent1.strip(), ent2.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_spans(spans):\n",
    "    \"\"\"Filter a sequence of spans and remove duplicates or overlaps. Useful for\n",
    "    creating named entities (where one token can only be part of one entity) or\n",
    "    when merging spans with `Retokenizer.merge`. When spans overlap, the (first)\n",
    "    longest span is preferred over shorter spans.\n",
    "    spans (iterable): The spans to filter.\n",
    "    RETURNS (list): The filtered spans.\n",
    "    \"\"\"\n",
    "    get_sort_key = lambda span: (span.end - span.start, span.start)\n",
    "    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n",
    "    result = []\n",
    "    seen_tokens = set()\n",
    "    for span in sorted_spans:\n",
    "        # Check for end - 1 here because boundaries are inclusive\n",
    "        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n",
    "            result.append(span)\n",
    "        seen_tokens.update(range(span.start, span.end))\n",
    "    result = sorted(result, key=lambda span: span.start)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_ent(ent, sent):\n",
    "    unwanted_tokens = (\n",
    "        'PRON',  # pronouns\n",
    "        'PART',  # particle\n",
    "        'DET',  # determiner\n",
    "        'SCONJ',  # subordinating conjunction\n",
    "        'PUNCT',  # punctuation\n",
    "        'SYM',  # symbol\n",
    "        'X',  # other\n",
    "        )\n",
    "    ent_type = ent.ent_type_  # get entity type\n",
    "    if ent_type == '':\n",
    "        ent_type = 'NOUN_CHUNK'\n",
    "        ent = ' '.join(str(t.text) for t in\n",
    "                nlp(str(ent)) if t.pos_\n",
    "                not in unwanted_tokens and t.is_stop == False)\n",
    "    elif ent_type in ('NOMINAL', 'CARDINAL', 'ORDINAL') and str(ent).find(' ') == -1:\n",
    "        t = ''\n",
    "        for i in range(len(sent) - ent.i):\n",
    "            if ent.nbor(i).pos_ not in ('VERB', 'PUNCT'):\n",
    "                t += ' ' + str(ent.nbor(i))\n",
    "            else:\n",
    "                ent = t.strip()\n",
    "                break\n",
    "    return ent, ent_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_pairs(text, coref=True):\n",
    "    text = re.sub(r'\\n+', '.', text)  # replace multiple newlines with period\n",
    "    text = re.sub(r'\\[\\d+\\]', ' ', text)  # remove reference numbers\n",
    "    text = nlp(text)\n",
    "    if coref:\n",
    "        text = nlp(text._.coref_resolved)  # resolve coreference clusters\n",
    "    sentences = [sent.string.strip() for sent in text.sents]  # split text into sentences\n",
    "    ent_pairs = list()\n",
    "    for sent in sentences:\n",
    "        sent = nlp(sent)\n",
    "        spans = list(sent.ents) + list(sent.noun_chunks)  # collect nodes\n",
    "        spans = filter_spans(spans)\n",
    "        with sent.retokenize() as retokenizer:\n",
    "            [retokenizer.merge(span) for span in spans]\n",
    "        dep = [token.dep_ for token in sent]\n",
    "        try:\n",
    "            if (dep.count('obj')+dep.count('dobj'))==1 \\\n",
    "                    and (dep.count('subj')+dep.count('nsubj'))==1:\n",
    "                for token in sent:\n",
    "                    if token.dep_ in ('obj', 'dobj'):  # identify object nodes\n",
    "                        subject = [w for w in token.head.lefts if w.dep_\n",
    "                                   in ('subj', 'nsubj')]  # identify subject nodes\n",
    "                        if subject:\n",
    "                            subject = subject[0]\n",
    "                            # identify relationship by root dependency\n",
    "                            relation = [w for w in token.ancestors if w.dep_ == 'ROOT']  \n",
    "                            if relation:\n",
    "                                relation = relation[0]\n",
    "                                # add adposition or particle to relationship\n",
    "                                if relation.nbor(1).pos_ in ('ADP', 'PART'):  \n",
    "                                    relation = ' '.join((str(relation),\n",
    "                                            str(relation.nbor(1))))\n",
    "                            else:\n",
    "                                relation = 'unknown'\n",
    "                            subject, subject_type = refine_ent(subject, sent)\n",
    "                            token, object_type = refine_ent(token, sent)\n",
    "                            ent_pairs.append([str(subject), str(relation), str(token),\n",
    "                                    str(subject_type), str(object_type)])\n",
    "        except:\n",
    "            print('error')\n",
    "    filtered_ent_pairs = [sublist for sublist in ent_pairs\n",
    "                          if not any(str(x) == '' for x in sublist)]\n",
    "    pairs = pd.DataFrame(filtered_ent_pairs, columns=['subject',\n",
    "                         'relation', 'object', 'subject_type',\n",
    "                         'object_type'])\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation(sent):\n",
    "\n",
    "    doc = nlp(sent)\n",
    "\n",
    "    # Matcher class object \n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    #define the pattern \n",
    "    pattern = [{'DEP':'ROOT'}, \n",
    "            {'DEP':'prep','OP':\"?\"},\n",
    "            {'DEP':'agent','OP':\"?\"},  \n",
    "            {'POS':'ADJ','OP':\"?\"}] \n",
    "\n",
    "    matcher.add(\"matching_1\", None, pattern) \n",
    "\n",
    "    matches = matcher(doc)\n",
    "    k = len(matches) - 1\n",
    "\n",
    "    span = doc[matches[k][1]:matches[k][2]] \n",
    "\n",
    "    return(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(text_list):\n",
    "    doc=nlp(text_list)\n",
    "    df=pd.DataFrame()\n",
    "    for sent in list(doc.sents):\n",
    "        sent = clean(str(sent))\n",
    "        try:\n",
    "            sub,obj = entity_pairs(str(sent), False)\n",
    "            relation= get_relation(str(sent))\n",
    "        except:\n",
    "            sub = None \n",
    "            obj = None\n",
    "            relation = None\n",
    "            \n",
    "        if ((relation not in (None, '')) & (sub not in (None, '')) &(obj not in (None, ''))):\n",
    "            df=df.append({'subject':sub,'relation':relation,'object':obj},ignore_index=True)\n",
    "            \n",
    "\n",
    "    df.drop_duplicates(keep = False, inplace = True)  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8428d0b2c1594cc59734ddfaa6fde09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676cd32154c642388f5c0eb69107f847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=885.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8424a6b3e9403ba67bb4a4ce5cb73d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9118.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642f0b40df624a39ab8027c1117bfbbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16959.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb5bd1a95814824861ce44cb62d68d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2353.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(files):\n",
    "    df = pd.DataFrame()\n",
    "    directory = file['dirpath']\n",
    "    for filenames in tqdm(file['filenames']):\n",
    "        with open(directory + '/' + filenames, \"r\") as read_file:\n",
    "            data = json.load(read_file)\n",
    "        \n",
    "        ID, title, abstract, text = pub_extract(data)\n",
    "        key_phrases = phrases_extract(abstract)[0:5]\n",
    "        abstract_tripples = entity_pairs(abstract, False).to_json()\n",
    "        #text_tripples = entity_pairs(text, False).to_json()\n",
    "        df=df.append({'ID':ID,\n",
    "                      'Repo':directory.split('/')[1],\n",
    "                      'Title':title,\n",
    "                      'Abstract':abstract,\n",
    "                      'Text':text,\n",
    "                      'Key Phrases':key_phrases,\n",
    "                      'Abs Tri': abstract_tripples },ignore_index=True)   \n",
    "        \n",
    "    df.to_csv(directory +'CleanResults_1.1.csv',    index=False, header=False,)\n",
    "    df.to_pickle(directory +'CleanResults_1.1.pickle')\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "Update nlp model with:\n",
    "\n",
    "[1] scispacy\n",
    "\n",
    "@inproceedings{Neumann2019ScispaCyFA,\n",
    "  title={ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing},\n",
    "  author={Mark Neumann and Daniel King and Iz Beltagy and Waleed Ammar},\n",
    "  year={2019},\n",
    "  Eprint={arXiv:1902.07669}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
