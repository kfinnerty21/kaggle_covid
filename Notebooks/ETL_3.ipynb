{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Entity and Topic Extraction\n",
    "\n",
    "This notebook will aim to extract important topics from documents all documents. \n",
    "The output will be a cleaned dataset containing all text content for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(r'c:\\users\\owner\\appdata\\roaming\\python\\python37\\site-packages')\n",
    "\n",
    "import scispacy\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from rake_nltk import Rake\n",
    "from spacy.matcher import Matcher \n",
    "import en_core_web_lg\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "nlp_1 = spacy.load('en_core_sci_lg')\n",
    "nlp = spacy.load('en_core_sci_lg')\n",
    "# Add the abbreviation pipe to the spacy pipeline.\n",
    "abbreviation_pipe = AbbreviationDetector(nlp_1)\n",
    "nlp.add_pipe(abbreviation_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify and load file directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['../data/biorxiv_medrxiv/', '../data/comm_use_subset/','../data/custom_license/', '../data/noncomm_use_subset/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for directory in directories:\n",
    "    for (dirpath, dirnames, filenames) in os.walk(directory):\n",
    "        filenames = [names for names in filenames if '.json' in names]\n",
    "        if filenames != []:\n",
    "            files.append({'dirpath':dirpath, 'filenames':filenames})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial string cleaning\n",
    "\n",
    "**TODO: improving text cleaning will provide fundamental improvements.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(txt):\n",
    "    txt=re.sub(r'\\n',' ',txt)\n",
    "    txt=re.sub(' +', ' ', txt)\n",
    "    txt=re.sub(',', ' ', txt)\n",
    "    txt=re.sub(r'\\([^()]*\\)','',txt)\n",
    "    txt=re.sub(r'https?:\\S+\\sdoi','',txt)\n",
    "    return txt.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data classes from each document. \n",
    "\n",
    "**TODO: Additional infromation available expanding this could further enrich datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pub_extract(data):\n",
    "    abstract = ''\n",
    "    for section in data['abstract']:\n",
    "        abstract = abstract + '  ' +  section['text']\n",
    "    abstract = clean(abstract)\n",
    "    \n",
    "    text = ''\n",
    "    for section in data['body_text']:\n",
    "        text = text + '  ' + section['text']\n",
    "    text = clean(text)\n",
    "    ID = data['paper_id']\n",
    "    title = data['metadata']['title']\n",
    "    return ID, title, abstract, text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple topic extraction can be used to cluster documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrases_extract(text):\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(text)\n",
    "    key_phrase = r.get_ranked_phrases() # To get keyword phrases ranked highest to lowest.\n",
    "    \n",
    "    return key_phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Extraction Functions\n",
    "\n",
    "Inspired by: https://towardsdatascience.com/auto-generated-knowledge-graphs-92ca99a81121\n",
    "\n",
    "**TODO: Investigate the potential of biomedical specific NER model [1.] **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_spans(spans):\n",
    "    \"\"\"Filter a sequence of spans and remove duplicates or overlaps. Useful for\n",
    "    creating named entities (where one token can only be part of one entity) or\n",
    "    when merging spans with `Retokenizer.merge`. When spans overlap, the (first)\n",
    "    longest span is preferred over shorter spans.\n",
    "    spans (iterable): The spans to filter.\n",
    "    RETURNS (list): The filtered spans.\n",
    "    \"\"\"\n",
    "    get_sort_key = lambda span: (span.end - span.start, span.start)\n",
    "    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n",
    "    result = []\n",
    "    seen_tokens = set()\n",
    "    for span in sorted_spans:\n",
    "        # Check for end - 1 here because boundaries are inclusive\n",
    "        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n",
    "            result.append(span)\n",
    "        seen_tokens.update(range(span.start, span.end))\n",
    "    result = sorted(result, key=lambda span: span.start)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_ent(ent, sent):\n",
    "    unwanted_tokens = (\n",
    "        'PRON',  # pronouns\n",
    "        'PART',  # particle\n",
    "        'DET',  # determiner\n",
    "        'SCONJ',  # subordinating conjunction\n",
    "        'PUNCT',  # punctuation\n",
    "        'X',  # other\n",
    "        )\n",
    "    ent_type = ent.ent_type_  # get entity type\n",
    "    if ent_type == '':\n",
    "        ent_type = 'NOUN_CHUNK'\n",
    "        ent = ' '.join(str(t.text) for t in\n",
    "                nlp(str(ent)) if t.pos_\n",
    "                not in unwanted_tokens and t.is_stop == False)\n",
    "    elif ent_type in ('NOMINAL', 'CARDINAL', 'ORDINAL') and str(ent).find(' ') == -1:\n",
    "        t = ''\n",
    "        for i in range(len(sent) - ent.i):\n",
    "            if ent.nbor(i).pos_ not in ('VERB', 'PUNCT'):\n",
    "                t += ' ' + str(ent.nbor(i))\n",
    "            else:\n",
    "                ent = t.strip()\n",
    "                break\n",
    "    return ent, ent_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_pairs(text, coref=True):\n",
    "    text = re.sub(r'\\n+', '.', text)  # replace multiple newlines with period\n",
    "    text = re.sub(r'\\[\\d+\\]', ' ', text)  # remove reference numbers\n",
    "    text = nlp(text)\n",
    "    if coref:\n",
    "        text = nlp(text._.coref_resolved)  # resolve coreference clusters\n",
    "    sentences = [sent.string.strip() for sent in text.sents]  # split text into sentences\n",
    "    ent_pairs = list()\n",
    "    for sent in sentences:\n",
    "        sent = nlp(sent)\n",
    "        spans = list(sent.ents) + list(sent.noun_chunks)  # collect nodes\n",
    "        spans = filter_spans(spans)\n",
    "        with sent.retokenize() as retokenizer:\n",
    "            [retokenizer.merge(span) for span in spans]\n",
    "        dep = [token.dep_ for token in sent]\n",
    "        try:\n",
    "            if (dep.count('obj')+dep.count('dobj'))==1 \\\n",
    "                    and (dep.count('subj')+dep.count('nsubj'))==1:\n",
    "                print('pass')\n",
    "                for token in sent:\n",
    "                    if token.dep_ in ('obj', 'dobj'):  # identify object nodes\n",
    "                        subject = [w for w in token.head.lefts if w.dep_\n",
    "                                   in ('subj', 'nsubj')]  # identify subject nodes\n",
    "                        if subject:\n",
    "                            subject = subject[0]\n",
    "                            # identify relationship by root dependency\n",
    "                            relation = [w for w in token.ancestors if w.dep_ == 'ROOT']  \n",
    "                            if relation:\n",
    "                                relation = relation[0]\n",
    "                                # add adposition or particle to relationship\n",
    "                                if relation.nbor(1).pos_ in ('ADP', 'PART'):  \n",
    "                                    relation = ' '.join((str(relation),\n",
    "                                            str(relation.nbor(1))))\n",
    "                            else:\n",
    "                                relation = 'unknown'\n",
    "                            subject, subject_type = refine_ent(subject, sent)\n",
    "                            token, object_type = refine_ent(token, sent)\n",
    "                            ent_pairs.append([str(subject), str(relation), str(token),\n",
    "                                    str(subject_type), str(object_type)])\n",
    "        except:\n",
    "            print('error')\n",
    "    filtered_ent_pairs = [sublist for sublist in ent_pairs\n",
    "                          if not any(str(x) == '' for x in sublist)]\n",
    "    pairs = pd.DataFrame(filtered_ent_pairs, columns=['subject',\n",
    "                         'relation', 'object', 'subject_type',\n",
    "                         'object_type'])\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Search\n",
    "\n",
    "Using keywords to link publication to Kaggle tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../tasks/keywords.json') as json_file:\n",
    "    keywords = json.load(json_file)\n",
    "\n",
    "def identify_tasks(text, keywords):\n",
    "    text = lemmatizer.lemmatize(text)\n",
    "    task_list=[]\n",
    "    for k,v in keywords.items(): \n",
    "        if any(x in text for x in v['keywords']):\n",
    "            task_list.append(k)  \n",
    "    return task_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply each of the techniques outline above to each publication and aggrigate findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=885.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[word count, 194 22 text word count]\n",
      "[all rights]\n",
      "[no reuse, permission]\n",
      "[abstract, the positive stranded rna genomes, picornaviruses, a single large open reading 28 frame, 5′, 3′ untranslated regions]\n",
      "[foot-and-mouth disease virus  29, 5′ utr  , five structural domains]\n",
      "[the 30 internal ribosome entry site, which, initiation, translation  , the cis-acting 31 replication element]\n",
      "[less well characterised structures, length, poly-c-tract, nucleotides, a series, tandemly, pseudoknots]\n",
      "[we, investigated, the structures, pks 34, primer extension  analysis, their contribution, genome replication, mutation, deletion, experiments]\n",
      "[36 shape, mutation, experiments, the importance, pk 37, function]\n",
      "[deletion experiments, pks, replication  , they, genomes, a competitive advantage]\n",
      "[39 replicons, full-length genomes, all pks, replication, rescued, genomes, less than one pk copy]\n",
      "[consistent with, report, the presence, putative packaging signals, pk region]\n",
      "[42 43 author/funder]\n",
      "[all rights]\n",
      "[no reuse, permission]\n",
      "[vp3  , vp0  ]\n",
      "[the p2 64 and p3 regions, non-structural proteins 2b, 2c, 3a  3b    3c pro, structural, protein-coding region, reporter genes, the study, genome, replication, requirement, high containment]\n",
      "[the fmdv 5′ utr, picornavirus utr  , approximately 1300 71 nucleotides, several highly structured regions]\n",
      "[the first 360 nucleotides, s-fragment  , pks, tandem repeats, nucleotide region, a small stem loop, downstream interaction site, sequence similarity, pks, it, they, duplication, events, viral replication  , recombination]\n",
      "[between two and four pks, virus, isolates, no strain, identified, pks  , their potential importance, viral life cycle]\n",
      "[the presence, pks, 5′ utr, picornaviruses]\n",
      "[all rights]\n",
      "[no reuse, permission, absence, pks, wild-type]\n",
      "[104 furthermore  competition experiments, extra copies, pks, a replicative 105 advantage, genomes]\n",
      "[replicons, full-length genomes, pks, replication-competent, infectious virus, rescued, genomes, less than one 107 pk copy]\n",
      "[consistent with, report, the presence, packaging signals, pk region]\n",
      "[109 110 author/funder]\n",
      "[all rights]\n",
      "[no reuse, permission]\n",
      "[plasmid construction]\n",
      "[117 the fmdv replicon plasmids, prep-ptgfp  and the replication-defective polymerase mutant 118 control]\n",
      "[mutations, pk region  , the prep-ptgfp replicon plasmid, spei, kpni, fragment, inserted, sub-cloning vector  , the pbluescript pk]\n",
      "[pks 3, removed, digestion, hindiii, insertion, synthetic dna sequence, pk 3]\n",
      "[pks 2  3, pcr amplification, δpk 234 forward primer, fmdv 1331-1311 reverse 125 primer  , digested, hindiii, aatii, ligated, pbluescript pk vector]\n",
      "[complete pk deletion, introduction, aflii site, the 3′ end, poly-c tract, pcr mutagenesis, sub-cloning vector  pbluescript 128 c11  , which, all the pks, pcr mutagenesis, δpk 1234 forward 129 primer, fmdv 1331-1311 reverse primer]\n",
      "[the modified pk sequences, the sub-cloning vectors, prep-ptgfp plasmid, nhei-hf, kpni-131 hf]\n",
      "[132 133 author/funder]\n",
      "[all rights]\n",
      "[no reuse, permission]\n",
      "[the copyright holder, preprint  ]\n",
      "[biorxiv preprint 7 mutations, disrupt, reform, pk structure, synthetic dna, digestion, aflii, aatii, ligation, digested, pk vector]\n",
      "[mutations, replicon plasmid]\n",
      "[assess, effects of, truncation, poly-c-tract, replication, the entire sequence, removed]\n",
      "[performed, pcr mutagenesis, primers c0 spei  , fmdv 1331-, in vitro, transcription]\n",
      "[143 in vitro transcription reactions, replicon assays]\n",
      "[transcription reactions, large amounts, rna, shape analysis, purified, 1 μg of linearised dna, dnase treatment, purification, a purelink fastq files, were quality, poor quality reads, the 225 sickle algorithm]\n",
      "[host cell reads, removed, fastq screen algorithm, fmdv 226 reads, de novo, contigs, idba-ud]\n",
      "[contigs, that, matched, the fmdv 227 library, identified, basic local alighnment search tool, 228 author/funder]\n",
      "[all rights]\n",
      "[no reuse, permission]\n",
      "[the copyright holder, preprint  ]\n",
      "[: biorxiv preprint, consensus sequences, seqman pro software, dna star lasergene 13 229 package  ]\n",
      "[the shape data, the predicted structures, the stems, pk 1  2 and 3  interacting nucleotides, little to no reactivity, nmia]\n",
      "[all rights]\n",
      "[no reuse, permission]\n",
      "[the copyright holder, preprint  ]\n",
      "[: biorxiv preprint, these nucleotides, base pairing, steric hindrance]\n",
      "[the nmia reactivity, interacting, nucleotides, stem-loops, downstream residues, the predicted structure, the shape data, fewer interactions, predicted]\n",
      "[differences, heterogeneity, formation, pks, experiment]\n",
      "[the evidence, loop-downstream, interaction, weaker, pk4]\n",
      "[the copyright holder, preprint  ]\n",
      "[: biorxiv preprint orientation]\n",
      "[removal, pks, decrease, replication  , requirements, wt levels, replication, investigated]\n",
      "[as near wt level, replication, only one pk, all further mutagenesis, c11 replicon, plasmid, pk 1]\n",
      "[addition, the orientation, pk 1, flipping, nucleotide sequence, hybridisation, loop, downstream sequences]\n",
      "[changing, the orientation, pk, reduced, replicon, replication, level, replication, decreased, passage, a 2.5 fold reduction]\n",
      "[all rights]\n",
      "[no reuse, permission]\n",
      "[the copyright holder, preprint  ]\n",
      "[: biorxiv preprint passage]\n",
      "[it, replicons, a competitive 399 disadvantage]\n",
      "[the copyright holder, preprint  ]\n",
      "[biorxiv preprint, infectious virus, replicate, transfection, cells  , with 448 a requirement, rna structure, pk region, virus assembly]\n",
      "[5′ utr, fmdv, picornaviruses, size, multiple rna elements, which, unknown function]\n",
      "[one of these features 455 is a series, repeated, pks, number, virus strain]\n",
      "[study, we, the pks, their role, viral life cycle]\n",
      "[we, the predicted pk structures, shape, mapping, all viruses, isolated, at least two pks  replicons, viruses, a 464 single pk, replication]\n",
      "[however  replicons, a competitive advantage, replicons, passaged]\n",
      "[replicons, all pks, poor passaging potential, co-467, transfected, yeast trna, reinforcing, the observation, significant, impact, replication]\n",
      "[viruses, recovered, genomes, reduced, pks, smaller plaques]\n",
      "[addition, these differences, pks, as no differences, replicons, a single 472 author/funder]\n",
      "[all rights]\n",
      "[no reuse, permission]\n",
      "[the copyright holder, preprint  ]\n",
      "[: biorxiv preprint 21 copy, pk1, pk4]\n",
      "[this observation, consistent with, deletion, pk1  , poly-c-tract  , adverse effect, viral replication]\n",
      "[our findings, the truncation, poly-c-tract, the c11 construct, no effect, replicon, replication, cell lines, tested]\n",
      "[mengo virus  , it, the role, poly-c-tract, viral lifecycle, which, standard tissue culture system]\n",
      "[the presence, pks, viral isolates, sequenced, multiple 480 pks, a competitive advantage, replication]\n",
      "[we, sequential passage, 481 replicons, at least two pks, level, replicons, only one pk, a persistent decline]\n",
      "[it, unclear, some viral isolates, four pks, this may be stochastic variation, subtle effects, host, geographical localisation]\n",
      "[the copyright holder, preprint  ]\n",
      "[: biorxiv preprint author/funder]\n",
      "[all rights]\n",
      "[no reuse, permission]\n",
      "[the copyright holder, preprint  ]\n",
      "[biorxiv preprint, the copyright holder, preprint  ]\n",
      "[biorxiv preprint significance, the replication, c11 pk, disrupt, c11 pk]\n",
      "[significance, compared, replicon]\n",
      "[error bars, calculated, by sem  n]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[674 author/funder]\n",
      "[all rights]\n",
      "[no reuse, permission]\n",
      "[the copyright holder, preprint  ]\n",
      "[: biorxiv preprint 33 675 author/funder]\n",
      "[all rights]\n",
      "[no reuse, permission]\n",
      "[the copyright holder, preprint  ]\n",
      "[: biorxiv preprint]\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(files):\n",
    "    df = pd.DataFrame()\n",
    "    directory = file['dirpath']\n",
    "    for filenames in tqdm(file['filenames']):\n",
    "        with open(directory + '/' + filenames, \"r\") as read_file:\n",
    "            data = json.load(read_file)\n",
    "        \n",
    "        ID, title, abstract, text = pub_extract(data)\n",
    "        try:\n",
    "            key_phrases = phrases_extract(abstract)[0:5]\n",
    "        except:\n",
    "            key_phrases = []\n",
    "            print('keyword error')\n",
    "        try:\n",
    "            abstract_tripples = entity_pairs(abstract, False).to_json()\n",
    "        except:\n",
    "            abstract_tripples = '{\"subject\":{},\"relation\":{},\"object\":{},\"subject_type\":{},\"object_type\":{}}'\n",
    "            print('abs error')\n",
    "        try:\n",
    "            text_tripples = entity_pairs(text, False).to_json()\n",
    "        except:\n",
    "            text_tripples = '{\"subject\":{},\"relation\":{},\"object\":{},\"subject_type\":{},\"object_type\":{}}'\n",
    "            print('keyword error')\n",
    "        try:\n",
    "            task_list = identify_tasks(text, keywords)\n",
    "        except:\n",
    "            task_list = []\n",
    "            print('task list error')        \n",
    "        \n",
    "        df=df.append({'ID':ID,\n",
    "                      'Repo':directory.split('/')[1],\n",
    "                      'Title':title,\n",
    "                      'Abstract':abstract,\n",
    "                      'Text':text,\n",
    "                      'Key Phrases':key_phrases,\n",
    "                      'Abs Tri': abstract_tripples,\n",
    "                      'Text Tri': text_tripples,\n",
    "                      'Task List':task_list},ignore_index=True) \n",
    "        break\n",
    "    break\n",
    "    df.to_csv(directory +'CleanResults_1.2.csv',    index=False, header=False,)\n",
    "    df.to_pickle(directory +'CleanResults_1.2.pickle')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "Update nlp model with:\n",
    "\n",
    "[1] scispacy\n",
    "\n",
    "@inproceedings{Neumann2019ScispaCyFA,\n",
    "  title={ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing},\n",
    "  author={Mark Neumann and Daniel King and Iz Beltagy and Waleed Ammar},\n",
    "  year={2019},\n",
    "  Eprint={arXiv:1902.07669}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"subject\":{},\"relation\":{},\"object\":{},\"subject_type\":{},\"object_type\":{}}'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tripples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"subject\":{\"0\":\"36 shape\"},\"relation\":{\"0\":\"confirmed\"},\"object\":{\"0\":\"importance\"},\"subject_type\":{\"0\":\"ENTITY\"},\"object_type\":{\"0\":\"NOUN_CHUNK\"}}'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_tripples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vp3  and vp0  .', 'the p2 64 and p3 regions encode the non-structural proteins 2b and 2c and 3a  3b    3c pro and 4 structural protein-coding region is replaced by reporter genes  allow the study of genome 68 replication without the requirement for high containment  .', 'the fmdv 5′ utr is the largest known picornavirus utr  comprising approximately 1300 71 nucleotides and containing several highly structured regions.', 'the first 360 nucleotides at the 5′ 72 end are predicted to fold into a single large stem loop termed the s-fragment  followed by a the pks were originally predicted in 1987 and consist of two to four tandem repeats of a ~48 86 nucleotide region containing a small stem loop and downstream interaction site  87 . due to the sequence similarity between the pks   it is speculated that they 88 were formed by duplication events during viral replication  probably involving recombination.', '89 between two and four pks are present in different virus isolates but no strain has been 90 identified with less than two pks  emphasising their potential importance in the viral life cycle 91  .', 'the presence of pks has been reported in the 5′ utr of other picornaviruses such as 92 author/funder.', 'all rights reserved.', 'no reuse allowed without permission. can occur in the absence of pks at least one is required for wild-type  replication.', '104 furthermore  competition experiments showed that extra copies of pks conferred a replicative 105 advantage to genomes.', 'although replicons and full-length genomes lacking pks were 106 replication-competent  no infectious virus was rescued from genomes containing less than one 107 pk copy.', 'this is consistent with our earlier report describing the presence of putative 108 packaging signals in the pk region .', '109 110 author/funder.', 'all rights reserved.', 'no reuse allowed without permission.', 'plasmid construction.', '117 the fmdv replicon plasmids  prep-ptgfp  and the replication-defective polymerase mutant 118 control  3d-gnn  have already been described .', 'to introduce mutations into the pk region  the prep-ptgfp replicon plasmid was digested 121 with spei and kpni and the resulting fragment inserted into a sub-cloning vector  122 to create the pbluescript pk.', 'pks 3 and 4 were removed by digestion with hindiii and aatii 123 before insertion of a synthetic dna sequence with pk 3 and 4 deleted.', 'pks 2  3 and 4 were 124 deleted by pcr amplification using δpk 234 forward primer and fmdv 1331-1311 reverse 125 primer  the resultant product was digested with hindiii and aatii and ligated into the 126 pbluescript pk vector.', 'complete pk deletion was achieved by introduction of an aflii site at 127 the 3′ end of the poly-c tract by pcr mutagenesis to create the sub-cloning vector  pbluescript 128 c11  which was then used to remove all the pks by pcr mutagenesis using δpk 1234 forward 129 primer and fmdv 1331-1311 reverse primer.', 'the modified pk sequences were removed from 130 the sub-cloning vectors and inserted into the prep-ptgfp plasmid using nhei-hf and kpni-131 hf.', '132 133 author/funder.', 'all rights reserved.', 'no reuse allowed without permission.', 'the copyright holder for this preprint  is the .', ': biorxiv preprint 7 mutations to disrupt and reform pk structure were introduced using synthetic dna by 134 digestion with aflii and aatii and ligation into a similarly digested pbluescript pk vector.', 'mutations were then introduced into the replicon plasmid as described above.', 'to assess the effects of truncation of the poly-c-tract on replication the entire sequence was 137 removed.', 'this was performed by pcr mutagenesis using primers c0 spei  and fmdv 1331- in vitro transcription.', '143 in vitro transcription reactions for replicon assays were performed as described previously .', 'transcription reactions to produce large amounts of rna for shape analysis were performed 145 with purified linear dna as described above  and 1 μg of linearised dna was then used in a 146 hiscribe t7 synthesis kit   before dnase treatment and purification using a purelink fastq files were quality checked using fastqc with poor quality reads filtered using the 225 sickle algorithm.', 'host cell reads were removed using fastq screen algorithm and fmdv 226 reads assembled de novo into contigs using idba-ud .', 'contigs that matched the fmdv 227 library (identified using basic local alighnment search tool ) were assembled 228 author/funder.', 'all rights reserved.', 'no reuse allowed without permission.', 'the copyright holder for this preprint  is the .', ': biorxiv preprint into consensus sequences using seqman pro software in the dna star lasergene 13 229 package  .', 'the shape data largely agreed with the predicted structures with the stems of pk 1  2 and 3  interacting nucleotides showed little to no reactivity  suggesting nmia could not interact with 300 author/funder.', 'all rights reserved.', 'no reuse allowed without permission.', 'the copyright holder for this preprint  is the .', ': biorxiv preprint 14 these nucleotides either due to the predicted base pairing or steric hindrance .', 'the nmia reactivity for the interacting nucleotides in the stem-loops with downstream residues of 302 pk 1  2 and 3 again largely agreed with the predicted structure  although the shape data 303 suggests that there might be fewer interactions than previously predicted.', 'however  differences 304 here could be due to heterogeneity in the formation of pks in this experiment.', 'the evidence 305 for loop-downstream interaction was weaker for pk4.', 'the copyright holder for this preprint  is the .', ': biorxiv preprint orientation.', '351 since removal of all four pks resulted in a significant decrease in replication  the minimal 352 requirements to maintain wt levels of replication were investigated.', 'as near wt level of 353 replication was observed when only one pk was present  all further mutagenesis was 354 performed in a c11 replicon plasmid containing only pk 1.', 'in addition  the orientation of pk 1 was reversed by \"flipping\" the nucleotide sequence to 367 potentially facilitate hybridisation of the loop with upstream rather than downstream sequences.', 'changing the orientation of the pk reduced replicon replication to a similar level seen in the replication decreased until at passage three there is a 2.5 fold reduction compared to that of 398 author/funder.', 'all rights reserved.', 'no reuse allowed without permission.', 'the copyright holder for this preprint  is the .', ': biorxiv preprint passage 0 .', 'therefore  it appears that replicons with a single pk are at a competitive 399 disadvantage compared to those with two or more.', 'the copyright holder for this preprint  is the .', ': biorxiv preprint 20 of infectious virus despite being able to replicate after transfection into cells  is consistent with 448 a requirement for rna structure within the pk region being required for virus assembly.', 'the 5′ utr of fmdv is unique amongst picornaviruses due to its large size and the presence 454 of multiple rna elements  some of which still have unknown function.', 'one of these features 455 is a series of repeated pks varying in number from 2-4  depending on virus strain.', 'in this study  456 we sequentially deleted or mutated the pks to help understand their role in the viral life cycle.', '457 we also confirmed the predicted pk structures by shape mapping  although there may be although all viruses isolated to date contain at least two pks  replicons or viruses containing a 464 single pk were still replication competent.', 'however  replicons with more than a single pk 465 were found to have a competitive advantage over replicons with a single pk when sequentially 466 passaged.', 'replicons lacking all pks displayed poor passaging potential even when co-467 transfected with yeast trna  reinforcing the observation of a significant impact in replication.', 'moreover  viruses recovered from genomes with reduced numbers of pks were slower growing 469 and produced smaller plaques.', 'in addition  these differences were more pronounced in more pks is functionally competent as no differences was seen between replicons congaing a single 472 author/funder.', 'all rights reserved.', 'no reuse allowed without permission.', 'the copyright holder for this preprint  is the .', ': biorxiv preprint 21 copy of pk1 or pk4.', 'this observation is consistent with a previous report of deletion of pk1  473 along with the poly-c-tract  with no adverse effect in viral replication .', 'this also supports 474 our findings that the truncation of the poly-c-tract to create the c11 construct had no effect on 475 replicon replication in the cell lines tested.', 'as has been described with mengo virus  it is 476 possible that the role of the poly-c-tract is essential in other aspects of the viral lifecycle which 477 cannot be recapitulated in a standard tissue culture system .', 'the presence of at least two pks in all viral isolates sequenced so far suggests that multiple 480 pks confer a competitive advantage in replication.', 'here we showed by sequential passage that 481 replicons containing at least two pks were maintained at a level similar to wt  but replicons 482 containing only one pk showed a persistent decline.', 'it is unclear why some viral isolates 483 contain two  three or four pks is still unknown  but this may be stochastic variation or may 484 reflect subtle effects of host range or geographical localisation.', 'the copyright holder for this preprint  is the .', ': biorxiv preprint author/funder.', 'all rights reserved.', 'no reuse allowed without permission.', 'the copyright holder for this preprint  is the .', ': biorxiv preprint the copyright holder for this preprint  is the .', ': biorxiv preprint significance is shown comparing the replication of c11 pk disrupt and c11 pk restore .', 'significance shown is compared to wt replicon.', 'error bars are calculated by sem  n = 3  *', 'p 673 < 0.05  *', '*', '*', '*', 'p < 0.0001.', '674 author/funder.', 'all rights reserved.', 'no reuse allowed without permission.', 'the copyright holder for this preprint  is the .', ': biorxiv preprint 33 675 author/funder.', 'all rights reserved.', 'no reuse allowed without permission.', 'the copyright holder for this preprint  is the .', ': biorxiv preprint']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "      <th>subject_type</th>\n",
       "      <th>object_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [subject, relation, object, subject_type, object_type]\n",
       "Index: []"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_pairs(text, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>relation</th>\n",
       "      <th>object</th>\n",
       "      <th>subject_type</th>\n",
       "      <th>object_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [subject, relation, object, subject_type, object_type]\n",
       "Index: []"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sS = \"\"\"the fmdv 5′ utr is the largest known picornavirus utr comprising approximately 1300 71 nucleotides\"\"\"\n",
    "entity_pairs(sS, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "det the\n",
      "compound fmdv\n",
      "compound 5′\n",
      "nsubj utr\n",
      "cop is\n",
      "det the\n",
      "amod largest\n",
      "amod known\n",
      "compound picornavirus\n",
      "ROOT utr\n",
      "acl comprising\n",
      "advmod approximately\n",
      "compound 1300\n",
      "nummod 71\n",
      "dobj nucleotides\n"
     ]
    }
   ],
   "source": [
    "sent = nlp(sS)\n",
    "for ent in sent:\n",
    "    print(ent.dep_, ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the fmdv 5′ utr\n",
      "approximately 1300 71 nucleotides\n",
      "several highly structured regions\n"
     ]
    }
   ],
   "source": [
    "for c in sent.noun_chunks:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
