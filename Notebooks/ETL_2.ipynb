{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Entity and Topic Extraction\n",
    "\n",
    "This notebook will aim to extract important topics from documents all documents. \n",
    "The output will be a cleaned dataset containing all text content for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from rake_nltk import Rake\n",
    "from spacy.matcher import Matcher \n",
    "import en_core_web_lg\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify and load file directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['../data/biorxiv_medrxiv/', '../data/comm_use_subset/','../data/custom_license/', '../data/noncomm_use_subset/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for directory in directories:\n",
    "    for (dirpath, dirnames, filenames) in os.walk(directory):\n",
    "        filenames = [names for names in filenames if '.json' in names]\n",
    "        if filenames != []:\n",
    "            files.append({'dirpath':dirpath, 'filenames':filenames})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial string cleaning\n",
    "\n",
    "**TODO: improving text cleaning will provide fundamental improvements.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(txt):\n",
    "    txt=re.sub(r'\\n',' ',txt)\n",
    "    txt=re.sub(' +', ' ', txt)\n",
    "    txt=re.sub(',', ' ', txt)\n",
    "    txt=re.sub(r'\\([^()]*\\)','',txt)\n",
    "    txt=re.sub(r'https?:\\S+\\sdoi','',txt)\n",
    "    return txt.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data classes from each document. \n",
    "\n",
    "**TODO: Additional infromation available expanding this could further enrich datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pub_extract(data):\n",
    "    abstract = ''\n",
    "    for section in data['abstract']:\n",
    "        abstract = abstract + '  ' +  section['text']\n",
    "    abstract = clean(abstract)\n",
    "    \n",
    "    text = ''\n",
    "    for section in data['body_text']:\n",
    "        text = text + '  ' + section['text']\n",
    "    text = clean(text)\n",
    "    ID = data['paper_id']\n",
    "    title = data['metadata']['title']\n",
    "    return ID, title, abstract, text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple topic extraction can be used to cluster documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrases_extract(text):\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(text)\n",
    "    key_phrase = r.get_ranked_phrases() # To get keyword phrases ranked highest to lowest.\n",
    "    \n",
    "    return key_phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Extraction Functions\n",
    "\n",
    "Inspired by: https://towardsdatascience.com/auto-generated-knowledge-graphs-92ca99a81121\n",
    "\n",
    "**TODO: Investigate the potential of biomedical specific NER model [1.] **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_spans(spans):\n",
    "    \"\"\"Filter a sequence of spans and remove duplicates or overlaps. Useful for\n",
    "    creating named entities (where one token can only be part of one entity) or\n",
    "    when merging spans with `Retokenizer.merge`. When spans overlap, the (first)\n",
    "    longest span is preferred over shorter spans.\n",
    "    spans (iterable): The spans to filter.\n",
    "    RETURNS (list): The filtered spans.\n",
    "    \"\"\"\n",
    "    get_sort_key = lambda span: (span.end - span.start, span.start)\n",
    "    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n",
    "    result = []\n",
    "    seen_tokens = set()\n",
    "    for span in sorted_spans:\n",
    "        # Check for end - 1 here because boundaries are inclusive\n",
    "        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n",
    "            result.append(span)\n",
    "        seen_tokens.update(range(span.start, span.end))\n",
    "    result = sorted(result, key=lambda span: span.start)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_ent(ent, sent):\n",
    "    unwanted_tokens = (\n",
    "        'PRON',  # pronouns\n",
    "        'PART',  # particle\n",
    "        'DET',  # determiner\n",
    "        'SCONJ',  # subordinating conjunction\n",
    "        'PUNCT',  # punctuation\n",
    "        'SYM',  # symbol\n",
    "        'X',  # other\n",
    "        )\n",
    "    ent_type = ent.ent_type_  # get entity type\n",
    "    if ent_type == '':\n",
    "        ent_type = 'NOUN_CHUNK'\n",
    "        ent = ' '.join(str(t.text) for t in\n",
    "                nlp(str(ent)) if t.pos_\n",
    "                not in unwanted_tokens and t.is_stop == False)\n",
    "    elif ent_type in ('NOMINAL', 'CARDINAL', 'ORDINAL') and str(ent).find(' ') == -1:\n",
    "        t = ''\n",
    "        for i in range(len(sent) - ent.i):\n",
    "            if ent.nbor(i).pos_ not in ('VERB', 'PUNCT'):\n",
    "                t += ' ' + str(ent.nbor(i))\n",
    "            else:\n",
    "                ent = t.strip()\n",
    "                break\n",
    "    return ent, ent_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_pairs(text, coref=True):\n",
    "    text = re.sub(r'\\n+', '.', text)  # replace multiple newlines with period\n",
    "    text = re.sub(r'\\[\\d+\\]', ' ', text)  # remove reference numbers\n",
    "    text = nlp(text)\n",
    "    if coref:\n",
    "        text = nlp(text._.coref_resolved)  # resolve coreference clusters\n",
    "    sentences = [sent.string.strip() for sent in text.sents]  # split text into sentences\n",
    "    ent_pairs = list()\n",
    "    for sent in sentences:\n",
    "        sent = nlp(sent)\n",
    "        spans = list(sent.ents) + list(sent.noun_chunks)  # collect nodes\n",
    "        spans = filter_spans(spans)\n",
    "        with sent.retokenize() as retokenizer:\n",
    "            [retokenizer.merge(span) for span in spans]\n",
    "        dep = [token.dep_ for token in sent]\n",
    "        try:\n",
    "            if (dep.count('obj')+dep.count('dobj'))==1 \\\n",
    "                    and (dep.count('subj')+dep.count('nsubj'))==1:\n",
    "                for token in sent:\n",
    "                    if token.dep_ in ('obj', 'dobj'):  # identify object nodes\n",
    "                        subject = [w for w in token.head.lefts if w.dep_\n",
    "                                   in ('subj', 'nsubj')]  # identify subject nodes\n",
    "                        if subject:\n",
    "                            subject = subject[0]\n",
    "                            # identify relationship by root dependency\n",
    "                            relation = [w for w in token.ancestors if w.dep_ == 'ROOT']  \n",
    "                            if relation:\n",
    "                                relation = relation[0]\n",
    "                                # add adposition or particle to relationship\n",
    "                                if relation.nbor(1).pos_ in ('ADP', 'PART'):  \n",
    "                                    relation = ' '.join((str(relation),\n",
    "                                            str(relation.nbor(1))))\n",
    "                            else:\n",
    "                                relation = 'unknown'\n",
    "                            subject, subject_type = refine_ent(subject, sent)\n",
    "                            token, object_type = refine_ent(token, sent)\n",
    "                            ent_pairs.append([str(subject), str(relation), str(token),\n",
    "                                    str(subject_type), str(object_type)])\n",
    "        except:\n",
    "            print('error')\n",
    "    filtered_ent_pairs = [sublist for sublist in ent_pairs\n",
    "                          if not any(str(x) == '' for x in sublist)]\n",
    "    pairs = pd.DataFrame(filtered_ent_pairs, columns=['subject',\n",
    "                         'relation', 'object', 'subject_type',\n",
    "                         'object_type'])\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Search\n",
    "\n",
    "Using keywords to link publication to Kaggle tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../tasks/keywords.json') as json_file:\n",
    "    keywords = json.load(json_file)\n",
    "\n",
    "def identify_tasks(text, keywords):\n",
    "    text = lemmatizer.lemmatize(text)\n",
    "    task_list=[]\n",
    "    for k,v in keywords.items(): \n",
    "        if any(x in text for x in v['keywords']):\n",
    "            task_list.append(k)  \n",
    "    return task_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply each of the techniques outline above to each publication and aggrigate findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=885.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9118.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "keyword error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16959.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "keyword error\n",
      "error\n",
      "error\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm(files):\n",
    "    df = pd.DataFrame()\n",
    "    directory = file['dirpath']\n",
    "    for filenames in tqdm(file['filenames']):\n",
    "        with open(directory + '/' + filenames, \"r\") as read_file:\n",
    "            data = json.load(read_file)\n",
    "        \n",
    "        ID, title, abstract, text = pub_extract(data)\n",
    "        try:\n",
    "            key_phrases = phrases_extract(abstract)[0:5]\n",
    "        except:\n",
    "            key_phrases = []\n",
    "            print('keyword error')\n",
    "        try:\n",
    "            abstract_tripples = entity_pairs(abstract, False).to_json()\n",
    "        except:\n",
    "            abstract_tripples = '{\"subject\":{},\"relation\":{},\"object\":{},\"subject_type\":{},\"object_type\":{}}'\n",
    "            print('abs error')\n",
    "        try:\n",
    "            text_tripples = entity_pairs(text, False).to_json()\n",
    "        except:\n",
    "            text_tripples = '{\"subject\":{},\"relation\":{},\"object\":{},\"subject_type\":{},\"object_type\":{}}'\n",
    "            print('keyword error')\n",
    "        try:\n",
    "            task_list = identify_tasks(text, keywords)\n",
    "        except:\n",
    "            task_list = []\n",
    "            print('task list error')        \n",
    "        \n",
    "        df=df.append({'ID':ID,\n",
    "                      'Repo':directory.split('/')[1],\n",
    "                      'Title':title,\n",
    "                      'Abstract':abstract,\n",
    "                      'Text':text,\n",
    "                      'Key Phrases':key_phrases,\n",
    "                      'Abs Tri': abstract_tripples,\n",
    "                      'Text Tri': text_tripples,\n",
    "                      'Task List':task_list},ignore_index=True)   \n",
    "    df.to_csv(directory +'CleanResults_1.2.csv',    index=False, header=False,)\n",
    "    df.to_pickle(directory +'CleanResults_1.2.pickle')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "Update nlp model with:\n",
    "\n",
    "[1] scispacy\n",
    "\n",
    "@inproceedings{Neumann2019ScispaCyFA,\n",
    "  title={ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing},\n",
    "  author={Mark Neumann and Daniel King and Iz Beltagy and Waleed Ammar},\n",
    "  year={2019},\n",
    "  Eprint={arXiv:1902.07669}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
